{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What the heck am I doing here: an Overview\n",
    "\n",
    "The first thing a new reader should do is to review the PDF in the attached git repository.\n",
    "\n",
    "There are three interesting components to this paper. I've included a quote below\n",
    "\n",
    "\"First we show that for a randomly\n",
    "initialized neural network with sufficiently many\n",
    "hidden units, the generic gradient descent algo-\n",
    "rithm learns any low degree polynomial, assum-\n",
    "ing we initialize the weights randomly. Secondly,\n",
    "we show that if we use complex-valued weights\n",
    "(the target function can still be real), then un-\n",
    "der suitable conditions, there are no “robust lo-\n",
    "cal minima”: the neural network can always es-\n",
    "cape a local minimum by performing a random\n",
    "perturbation. This property does not hold for\n",
    "real-valued weights. Thirdly, we discuss whether\n",
    "sparse polynomials can be learned with small\n",
    "neural networks, with the size dependent on the\n",
    "sparsity of the target function.\" - Learning Polynomials with Neural Networks\n",
    "\n",
    "The goal of the paper was to present some evidence supporting the effectiveness of neural networks, and I wish to further supplement that evidence by investigating some of their claims in this notebook.\n",
    "\n",
    "## Result 1\n",
    "\n",
    "We wish to investigate if, \"for a randomly initialized neural network with sufficiently many hidden units, the generic gradient descent algorithm learns any low degree polynomial, assuming we initialize the weights randomly.\"\n",
    "\n",
    "The most trivial polynomial is the constant zero. However, trying to verify that a neural network can duplicate this result is like trying to use a flamethrower to light a birthday candle.\n",
    "\n",
    "Recall the universal approximation theorem: \n",
    "https://en.wikipedia.org/wiki/Universal_approximation_theorem\n",
    "\n",
    "If the function we wish to model is constant, I think it follows trivially from this that a neural network is sufficiently powerful. Specifically, setting all the weights and biases to zero, and using any typical, sigmoidal, activation function should do the trick.\n",
    "\n",
    "Let's use a slightly less trivial example.\n",
    "\n",
    "f_1(x) = 2x+1\n",
    "The approximating power of neural networks is limited to compact sets. Hence, let us choose an interval of interest. Reasonably, I'd like an interval that contains both positive and negative inputs and outputs.\n",
    "I select the interval x in [-1,1]. f_1( [-1,1] ) = [-1, 3]. \n",
    "Given that the interval contains relatively small numbers, I don't an explicit normalization is necessary.",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
